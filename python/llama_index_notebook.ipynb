{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Dropbox data to use for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index\n",
    "%pip install azure-core\n",
    "%pip install pdf2image\n",
    "%pip install pandas \n",
    "%pip install matplotlib\n",
    "%pip install opencv-python\n",
    "%pip install Pillow\n",
    "%pip install azure-cognitiveservices-vision-computervision\n",
    "%pip install azure-cognitiveservices-search-imagesearch\n",
    "%pip install azure-cognitiveservices-search-newssearch\n",
    "%pip install numpy\n",
    "%pip install azure-cognitiveservices-search-websearch\n",
    "%pip install tabulate\n",
    "%pip install html2text\n",
    "\n",
    "# Hide output and messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set text wrapping\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def set_css():\n",
    "  display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import download_loader\n",
    "from pathlib import Path\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UnstructuredReader = download_loader(\"UnstructuredReader\", refresh_cache=True)\n",
    "loader = UnstructuredReader()\n",
    "doc_set = {}\n",
    "all_docs = []\n",
    "years = [2023]\n",
    "for year in years:\n",
    "    ubs_docs = loader.load_data(file=Path(f'../data/full-report-ubs-group-ag-consolidated-1q23.pdf'), split_documents=False)\n",
    "    # insert year metadata into each year\n",
    "    for d in ubs_docs:\n",
    "        d.extra_info = {\"year\": year}\n",
    "    doc_set[year] = ubs_docs\n",
    "    all_docs.extend(ubs_docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Service Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext, GPTVectorStoreIndex, GPTTreeIndex\n",
    "service_context = ServiceContext.from_defaults(chunk_size_limit=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize simple vector indices + global vector index\n",
    "# NOTE: don't run this cell if the indices are already loaded! \n",
    "index_set = {}\n",
    "for year in years:\n",
    "    cur_index = GPTVectorStoreIndex.from_documents(doc_set[year], service_context=service_context)\n",
    "    index_set[year] = cur_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(index_set[2023])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_set[2023].set_index_id(\"chatmyapp\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstrate MONGODB connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTListIndex, SimpleMongoReader\n",
    "from IPython.display import Markdown, display\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = \"<host>\"\n",
    "port = \"<port>\"\n",
    "db_name = \"<db_name>\"\n",
    "collection_name = \"<collection_name>\"\n",
    "# query_dict is passed into db.collection.find()\n",
    "query_dict = {}\n",
    "field_names = [\"text\"]\n",
    "reader = SimpleMongoReader(host, port)\n",
    "documents = reader.load_data(db_name, collection_name, field_names, query_dict=query_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_dict = {}\n",
    "field_names = [\"text\"]\n",
    "reader = SimpleMongoReader(host, port)\n",
    "\n",
    "# documents = reader.load_data(db_name, collection_name, field_names, query_dict=query_dict)\n",
    "# set Logging to DEBUG for more detailed outputs\n",
    "query_engine = new_var.as_query_engine()\n",
    "response = query_engine.query(\"summarize financial results for the quarter\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slack data connector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTListIndex, SlackReader\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slack_token = os.getenv(\"SLACK_BOT_TOKEN\")\n",
    "channel_ids = [\"<channel_id>\"]\n",
    "documents = SlackReader(slack_token=slack_token).load_data(channel_ids=channel_ids)\n",
    "index = GPTListIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"<query_text>\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Page Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTListIndex, SimpleWebPageReader\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data([\"http://paulgraham.com/worked.html\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"<b>{documents[0]}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index import ServiceContext, GPTVectorStoreIndex\n",
    "\n",
    "service_context = ServiceContext.from_defaults(chunk_size_limit=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 30776 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 30776 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 30776 tokens\n"
     ]
    }
   ],
   "source": [
    "global_index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 23 tokens\n",
      "> [retrieve] Total embedding token usage: 23 tokens\n",
      "> [retrieve] Total embedding token usage: 23 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1159 tokens\n",
      "> [get_response] Total LLM token usage: 1159 tokens\n",
      "> [get_response] Total LLM token usage: 1159 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "query_engine = global_index.as_query_engine()\n",
    "response = query_engine.query(\"List bullet points summarizing this text. Do not reapeat the author at the beginning of each bullet point.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "- Explored how author chose what to work on in the past\n",
       "- Discovered answer was long and messy\n",
       "- Thought others might find it interesting and encouraging\n",
       "- Wrote a more detailed version for others to read\n",
       "- Noted experience skipped step in evolution of computers\n",
       "- Italian words for abstract concepts can be predicted from English cognates\n",
       "- Described walk to Accademia in Florence\n",
       "- Noted painting people like still lives\n",
       "- Explained how Lisp was better than other languages\n",
       "- Noted difference between putting something online and publishing it online\n",
       "- Noted customs continue to constrain even after restrictions that caused them have disappeared\n",
       "- Noted independent-minded people will have an advantage in fields affected by rapid change\n",
       "- Noted can't always predict which fields will be affected by rapid change</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 16 tokens\n",
      "> [retrieve] Total embedding token usage: 16 tokens\n",
      "> [retrieve] Total embedding token usage: 16 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1082 tokens\n",
      "> [get_response] Total LLM token usage: 1082 tokens\n",
      "> [get_response] Total LLM token usage: 1082 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "The author explained the difference between putting something online and publishing it online by noting that in the print era, there was a narrow channel to readers, guarded by fierce monsters known as editors. The only way to get an audience for anything you wrote was to get it published as a book, or in a newspaper or magazine. Now, with the internet, anyone could publish anything. This meant that there would be a whole new generation of essays that had never been written before because there had been no way to publish them.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(\"How did the author explain the difference between putting something online and publishing it online?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composing a Graph to synthesize answers across documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index import GPTListIndex, LLMPredictor\n",
    "from langchain import OpenAI\n",
    "from llama_index.composability import ComposableGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set number of output tokens\n",
    "llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, max_tokens=512))\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatty-app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
